{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.config.model_args import ModelArgs\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import torch\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import sys\n",
    "import importlib\n",
    "import pathlib\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.functional import Tensor\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from torch.utils.data import sampler, DataLoader, TensorDataset\n",
    "from typing import Dict, List, Sequence, Set, Text, Tuple, Union\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, fbeta_score, accuracy_score\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "from imblearn.metrics import geometric_mean_score \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from simpletransformers.config.model_args import ModelArgs\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def pickle_data(data, file_name):\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "        \n",
    "def unpickle_data(data):\n",
    "    with open(data, \"rb\") as file:\n",
    "        loaded = pickle.load(file)\n",
    "    return loaded\n",
    "\n",
    "def reader(root: str, file: str):\n",
    "    with open(os.path.join(root, file), \"r\") as f: \n",
    "        return f.readlines()\n",
    "\n",
    "def json_r(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        loaded = json.load(file)\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "falcon-80 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/falcon-80/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1196\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     )\n\u001b[0;32m-> 1541\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6508ea92-225e67a242b8db7634d82875;e5f44905-48ed-4d62-a0a4-f2a18d70dda5)\n\nRepository Not Found for url: https://huggingface.co/falcon-80/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-089fd95451dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = RepresentationModel(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"falcon-80\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         use_cuda = False)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/simpletransformers/language_representation/representation_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_type, model_name, args, use_cuda, cuda_device, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mconfig_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_CLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             logger.warning(\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: falcon-80 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "model = RepresentationModel(\n",
    "        model_type = \"gpt2\",\n",
    "        model_name = \"gpt2\",\n",
    "        use_cuda = False)\n",
    "\n",
    "# test_string = [\"I am not\", \"in the\"]\n",
    "# test_string = [\"I am not\", \"in the\", \"mood but\", \"I am willing\", \"bulch if given\", \"what I want\"]\n",
    "\n",
    "# batch = 2\n",
    "\n",
    "# while True:\n",
    "#     test_string[:batch]\n",
    "    \n",
    "# for idx, data in enumerate(test_string):\n",
    "    \n",
    "# out_res = []\n",
    "# for data in test_string:\n",
    "#     temp_res = model.encode_sentences(data, combine_strategy = \"mean\", batch_size = len(data))\n",
    "#     out_res.append(temp_res)\n",
    "\n",
    "# len(out_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2a_train_1 = unpickle_data(\"/Users/Gabriel/Projects/vReduce/vreduce/features/d2a/one.pkl\")\n",
    "# d2a_train_2 = unpickle_data(\"/Users/Gabriel/Projects/vReduce/vreduce/features/d2a/two.pkl\")\n",
    "# d2a_train_3 = unpickle_data(\"/Users/Gabriel/Projects/vReduce/vreduce/features/d2a/three.pkl\")\n",
    "# d2a_train_4 = unpickle_data(\"/Users/Gabriel/Projects/vReduce/vreduce/features/d2a/four.pkl\")\n",
    "# d2a_train_5 = unpickle_data(\"/Users/Gabriel/Projects/vReduce/vreduce/features/d2a/five.pkl\")\n",
    "\n",
    "# # Get Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am', 'in the']\n",
      "['mood but', 'I am willing']\n",
      "['bulch if', 'what I want']\n"
     ]
    }
   ],
   "source": [
    "test_string = [\"I am\", \"in the\", \"mood but\", \"I am willing\", \"bulch if\", \"what I want\"]\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "# data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # list of data \n",
    "\n",
    "for x in batch(test_string, 2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "tiiuae/falcon-180b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/tiiuae/falcon-180b/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1196\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     )\n\u001b[0;32m-> 1541\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    276\u001b[0m             )\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-6508f6f3-2701d59d0e0864070cc1495f;d3e11537-6372-4b7c-b64c-44a15a7c5fa5)\n\nCannot access gated repo for url https://huggingface.co/tiiuae/falcon-180b/resolve/main/tokenizer_config.json.\nRepo model tiiuae/falcon-180B is gated. You must be authenticated to access it.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-5fc8ee6b95c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tiiuae/falcon-180b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m pipeline = transformers.pipeline(\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m     ```\"\"\"\n\u001b[1;32m    486\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    488\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: tiiuae/falcon-180b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-180b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x10e7eab20>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import pairwise\n",
    "# list(pairwise(test_string))\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not\n",
      "mood but\n",
      "bulch if given\n"
     ]
    }
   ],
   "source": [
    "test_string = [\"I am not\", \"in the\", \"mood but\", \"I am willing\", \"bulch if given\", \"what I want\"]\n",
    "for i in range(0, len(test_string), 2):\n",
    "    print(test_string[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RepresentationModel(\n",
    "        model_type = \"gpt2\",\n",
    "        model_name = \"gpt2\",\n",
    "        use_cuda = False)\n",
    "\n",
    "# =========== stmts starts (use all instead)\n",
    "oneK_d2a_stmts = model.encode_sentences(d2a_stmts_sequence[:1000], combine_strategy = \"mean\", batch_size = len(d2a_stmts_sequence[:1000]))\n",
    "pickle_data(oneK_d2a_stmts, \"oneK_d2a_stmts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Gabriel/Projects/OuR/our/features/dev/dev_d2a_stmts.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-370480fe5b42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md2a_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/Gabriel/Projects/OUR/data/d2a/function/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/Gabriel/Projects/OuR/our/features/dev/dev_d2a_stmts.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/Gabriel/Projects/OuR/our/features/dev/dev_d2a_decls.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mthree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/Gabriel/Projects/OuR/our/features/dev/dev_d2a_decls.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-75d1cb8eae91>\u001b[0m in \u001b[0;36munpickle_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/Gabriel/Projects/OuR/our/features/dev/dev_d2a_stmts.pkl'"
     ]
    }
   ],
   "source": [
    "d2a_train = pd.read_csv(\"/Users/Gabriel/Projects/OUR/data/d2a/function/train.csv\")\n",
    "d2a_dev = pd.read_csv(\"/Users/Gabriel/Projects/OUR/data/d2a/function/dev.csv\")\n",
    "d2a_test = pd.read_csv(\"/Users/Gabriel/Projects/OUR/data/d2a/function/test.csv\")\n",
    "\n",
    "one = unpickle_data(\"/Users/Gabriel/Projects/OuR/our/features/dev/dev_d2a_stmts.pkl\")\n",
    "two = unpickle_data(\"/Users/Gabriel/Projects/OuR/our/features/dev/dev_d2a_decls.pkl\")\n",
    "three = unpickle_data(\"/Users/Gabriel/Projects/OuR/our/features/dev/dev_d2a_decls.pkl\")\n",
    "\n",
    "d2a_dev_features = np.concatenate((one, two, three), axis = 1)\n",
    "\n",
    "# Data split = 80% train, 10% dev, 10% test\n",
    "d2a_train = pd.read_csv(\"/Users/Gabriel/Projects/OUR/data/d2a/function/train.csv\")\n",
    "d2a_dev = pd.read_csv(\"/Users/Gabriel/Projects/OUR/data/d2a/function/dev.csv\")\n",
    "\n",
    "d2a_train_labels = np.array(list(d2a_train[\"label\"]))\n",
    "d2a_dev_labels = np.array(list(d2a_dev[\"label\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2ForTextRepresentation: ['h.3.attn.c_attn.weight', 'h.9.ln_2.bias', 'h.10.attn.c_proj.bias', 'h.3.attn.c_attn.bias', 'h.1.ln_2.weight', 'h.2.mlp.c_fc.bias', 'h.7.mlp.c_fc.weight', 'h.3.attn.bias', 'h.0.attn.c_proj.weight', 'h.7.attn.bias', 'h.6.mlp.c_proj.bias', 'h.1.ln_1.bias', 'h.6.mlp.c_proj.weight', 'h.0.attn.c_proj.bias', 'h.2.attn.c_attn.weight', 'h.9.attn.c_proj.weight', 'h.11.mlp.c_fc.bias', 'h.11.mlp.c_proj.weight', 'h.0.attn.c_attn.bias', 'h.8.ln_2.bias', 'h.4.ln_1.weight', 'h.0.ln_2.bias', 'h.9.mlp.c_fc.weight', 'h.3.ln_1.weight', 'h.4.attn.bias', 'h.1.mlp.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.4.ln_1.bias', 'h.4.attn.c_attn.weight', 'h.8.ln_1.bias', 'h.6.attn.bias', 'h.11.ln_1.weight', 'h.6.attn.c_attn.weight', 'h.9.ln_1.weight', 'h.6.attn.c_proj.weight', 'h.10.attn.c_attn.bias', 'h.8.attn.c_proj.bias', 'h.9.attn.c_attn.weight', 'h.9.mlp.c_proj.weight', 'h.11.ln_2.bias', 'h.7.attn.c_proj.weight', 'h.10.ln_2.weight', 'h.4.mlp.c_proj.bias', 'h.1.attn.c_proj.bias', 'h.4.mlp.c_fc.bias', 'h.8.ln_2.weight', 'h.10.ln_2.bias', 'h.2.ln_2.weight', 'h.5.attn.c_proj.bias', 'h.8.attn.c_attn.weight', 'h.7.ln_2.bias', 'h.4.mlp.c_fc.weight', 'h.3.mlp.c_fc.bias', 'h.1.mlp.c_fc.weight', 'h.0.mlp.c_fc.weight', 'h.10.attn.c_attn.weight', 'h.3.mlp.c_fc.weight', 'h.0.ln_1.bias', 'h.6.attn.c_attn.bias', 'h.1.attn.bias', 'h.11.ln_2.weight', 'h.2.mlp.c_fc.weight', 'h.3.attn.c_proj.bias', 'h.5.mlp.c_fc.bias', 'h.0.attn.bias', 'h.8.mlp.c_proj.bias', 'wpe.weight', 'h.2.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'wte.weight', 'h.3.mlp.c_proj.bias', 'h.11.mlp.c_fc.weight', 'h.10.ln_1.weight', 'h.7.ln_2.weight', 'h.10.ln_1.bias', 'h.10.attn.bias', 'h.2.attn.c_proj.bias', 'h.5.ln_1.bias', 'h.10.mlp.c_fc.weight', 'h.9.mlp.c_proj.bias', 'h.11.attn.bias', 'h.1.mlp.c_proj.bias', 'h.11.mlp.c_proj.bias', 'h.6.ln_1.bias', 'h.0.attn.c_attn.weight', 'h.9.mlp.c_fc.bias', 'h.4.attn.c_attn.bias', 'h.7.mlp.c_fc.bias', 'h.1.mlp.c_fc.bias', 'ln_f.bias', 'h.8.mlp.c_fc.weight', 'h.1.ln_1.weight', 'h.2.mlp.c_proj.weight', 'h.5.attn.c_proj.weight', 'h.9.attn.bias', 'h.0.ln_2.weight', 'h.8.ln_1.weight', 'h.1.ln_2.bias', 'h.11.attn.c_proj.weight', 'h.7.ln_1.weight', 'h.5.mlp.c_proj.weight', 'ln_f.weight', 'h.7.ln_1.bias', 'h.10.mlp.c_proj.bias', 'h.7.attn.c_proj.bias', 'h.7.mlp.c_proj.bias', 'h.7.attn.c_attn.weight', 'h.8.attn.c_attn.bias', 'h.1.attn.c_attn.bias', 'h.9.attn.c_proj.bias', 'h.9.ln_1.bias', 'h.5.attn.c_attn.weight', 'h.6.attn.c_proj.bias', 'h.2.attn.c_proj.weight', 'h.3.ln_2.weight', 'h.6.ln_2.bias', 'h.10.mlp.c_fc.bias', 'h.11.attn.c_proj.bias', 'h.6.mlp.c_fc.bias', 'h.2.ln_1.weight', 'h.8.attn.bias', 'h.4.attn.c_proj.weight', 'h.7.mlp.c_proj.weight', 'h.4.ln_2.weight', 'h.4.mlp.c_proj.weight', 'h.6.ln_2.weight', 'h.3.ln_2.bias', 'h.7.attn.c_attn.bias', 'h.2.attn.bias', 'h.3.mlp.c_proj.weight', 'h.5.attn.bias', 'h.5.ln_2.bias', 'h.0.mlp.c_fc.bias', 'h.2.mlp.c_proj.bias', 'h.2.ln_2.bias', 'h.8.mlp.c_proj.weight', 'h.9.attn.c_attn.bias', 'h.5.ln_2.weight', 'h.0.mlp.c_proj.weight', 'h.6.ln_1.weight', 'h.1.attn.c_proj.weight', 'h.9.ln_2.weight', 'h.8.mlp.c_fc.bias', 'h.4.ln_2.bias', 'h.2.ln_1.bias', 'h.11.attn.c_attn.bias', 'h.5.ln_1.weight', 'h.8.attn.c_proj.weight', 'h.0.mlp.c_proj.bias', 'h.5.mlp.c_proj.bias', 'h.3.attn.c_proj.weight', 'h.10.mlp.c_proj.weight', 'h.11.attn.c_attn.weight', 'h.0.ln_1.weight', 'h.6.mlp.c_fc.weight', 'h.3.ln_1.bias', 'h.5.attn.c_attn.bias', 'h.10.attn.c_proj.weight', 'h.11.ln_1.bias', 'h.5.mlp.c_fc.weight']\n",
      "- This IS expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForTextRepresentation were not initialized from the model checkpoint at gpt2 and are newly initialized: ['gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.2.ln_2.weight', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.1.ln_1.weight', 'gpt2.h.6.attn.c_proj.bias', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.10.ln_1.bias', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.8.ln_1.bias', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.2.ln_1.weight', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.4.ln_1.weight', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.6.ln_2.bias', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.h.5.ln_1.bias', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.2.ln_1.bias', 'gpt2.h.0.ln_2.weight', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.7.ln_1.weight', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.9.ln_2.bias', 'gpt2.h.0.ln_1.weight', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.8.ln_1.weight', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.h.6.ln_1.weight', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.10.ln_2.weight', 'gpt2.h.4.ln_1.bias', 'gpt2.h.10.ln_1.weight', 'gpt2.h.7.ln_2.bias', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.3.ln_1.bias', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.4.ln_2.bias', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.5.ln_2.bias', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.wpe.weight', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.6.ln_2.weight', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.1.ln_2.bias', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.3.ln_2.bias', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.11.ln_2.bias', 'gpt2.h.9.ln_1.weight', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.3.ln_1.weight', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.10.ln_2.bias', 'gpt2.h.3.ln_2.weight', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.4.ln_2.weight', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.6.ln_1.bias', 'gpt2.h.9.ln_1.bias', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.11.ln_1.bias', 'gpt2.h.4.attn.c_attn.bias', 'gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.11.ln_2.weight', 'gpt2.wte.weight', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.5.ln_1.weight', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.8.ln_2.weight', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.11.ln_1.weight', 'gpt2.ln_f.weight', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.h.1.ln_1.bias', 'gpt2.h.7.ln_1.bias', 'gpt2.h.8.ln_2.bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.7.ln_2.weight', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.5.attn.c_proj.weight', 'gpt2.h.9.ln_2.weight', 'gpt2.h.0.ln_1.bias', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.1.ln_2.weight', 'gpt2.h.5.ln_2.weight', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.1.mlp.c_fc.weight', 'gpt2.ln_f.bias', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.2.ln_2.bias', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.9.attn.c_attn.weight', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.0.ln_2.bias', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.7.mlp.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# len(d2a_train)\n",
    "\n",
    "model = RepresentationModel(\n",
    "        model_type = \"gpt2\",\n",
    "        model_name = \"gpt2\",\n",
    "        use_cuda = False)\n",
    "\n",
    "# one = model.encode_sentences(devign_func[:1000], combine_strategy = \"mean\", batch_size = len(devign_func[:1000]))\n",
    "# pickle_data(one, \"one.pkl\")\n",
    "\n",
    "# one = model.encode_sentences(devign_func[:1000], combine_strategy = \"mean\", batch_size = len(devign_func[:1000]))\n",
    "# pickle_data(one, \"one.pkl\")\n",
    "\n",
    "# two = model.encode_sentences(devign_func[1000:2000], combine_strategy = \"mean\", batch_size = len(devign_func[1000:2000]))\n",
    "# pickle_data(two, \"two.pkl\")\n",
    "\n",
    "# three = model.encode_sentences(devign_func[2000:3000], combine_strategy = \"mean\", batch_size = len(devign_func[2000:3000]))\n",
    "# pickle_data(three, \"three.pkl\")\n",
    "\n",
    "# four = model.encode_sentences(devign_func[3000:4000], combine_strategy = \"mean\", batch_size = len(devign_func[3000:4000]))\n",
    "# pickle_data(four, \"four.pkl\")\n",
    "\n",
    "# five = model.encode_sentences(devign_func[4000:5000], combine_strategy = \"mean\", batch_size = len(devign_func[4000:5000]))\n",
    "# pickle_data(five, \"five.pkl\")\n",
    "\n",
    "# six = model.encode_sentences(devign_func[5000:6000], combine_strategy = \"mean\", batch_size = len(devign_func[5000:6000]))\n",
    "# pickle_data(six, \"six.pkl\")\n",
    "\n",
    "# seven = model.encode_sentences(devign_func[6000:7000], combine_strategy = \"mean\", batch_size = len(devign_func[6000:7000]))\n",
    "# pickle_data(seven, \"seven.pkl\")\n",
    "\n",
    "# eight = model.encode_sentences(devign_func[7000:8000], combine_strategy = \"mean\", batch_size = len(devign_func[7000:8000]))\n",
    "# pickle_data(eight, \"eight.pkl\")\n",
    "\n",
    "# nine = model.encode_sentences(devign_func[8000:9000], combine_strategy = \"mean\", batch_size = len(devign_func[8000:9000]))\n",
    "# pickle_data(nine, \"nine.pkl\")\n",
    "\n",
    "# ten = model.encode_sentences(devign_func[9000:10000], combine_strategy = \"mean\", batch_size = len(devign_func[9000:10000]))\n",
    "# pickle_data(ten, \"ten.pkl\")\n",
    "\n",
    "# eleven = model.encode_sentences(devign_func[10000:11000], combine_strategy = \"mean\", batch_size = len(devign_func[10000:11000]))\n",
    "# pickle_data(eleven, \"eleven.pkl\")\n",
    "\n",
    "# twelve = model.encode_sentences(devign_func[11000:12000], combine_strategy = \"mean\", batch_size = len(devign_func[11000:12000]))\n",
    "# pickle_data(twelve, \"twelve.pkl\")\n",
    "\n",
    "# thirteen = model.encode_sentences(devign_func[12000:13000], combine_strategy = \"mean\", batch_size = len(devign_func[12000:13000]))\n",
    "# pickle_data(thirteen, \"thirteen.pkl\")\n",
    "\n",
    "# fourteen = model.encode_sentences(devign_func[13000:14000], combine_strategy = \"mean\", batch_size = len(devign_func[13000:14000]))\n",
    "# pickle_data(fourteen, \"fourteen.pkl\")\n",
    "\n",
    "# fiveteen = model.encode_sentences(devign_func[14000:15000], combine_strategy = \"mean\", batch_size = len(devign_func[14000:15000]))\n",
    "# pickle_data(fiveteen, \"fiveteen.pkl\")\n",
    "\n",
    "# sixteen = model.encode_sentences(devign_func[15000:16000], combine_strategy = \"mean\", batch_size = len(devign_func[15000:16000]))\n",
    "# pickle_data(sixteen, \"sixteen.pkl\")\n",
    "\n",
    "# seventeen = model.encode_sentences(devign_func[16000:17000], combine_strategy = \"mean\", batch_size = len(devign_func[16000:17000]))\n",
    "# pickle_data(seventeen, \"seventeen.pkl\")\n",
    "\n",
    "# eightteen = model.encode_sentences(devign_func[17000:18000], combine_strategy = \"mean\", batch_size = len(devign_func[17000:18000]))\n",
    "# pickle_data(eightteen, \"eightteen.pkl\")\n",
    "\n",
    "# nineteen = model.encode_sentences(devign_func[18000:19000], combine_strategy = \"mean\", batch_size = len(devign_func[18000:19000]))\n",
    "# pickle_data(nineteen, \"nineteen.pkl\")\n",
    "\n",
    "# twenty = model.encode_sentences(devign_func[19000:20000], combine_strategy = \"mean\", batch_size = len(devign_func[19000:20000]))\n",
    "# pickle_data(twenty, \"twenty.pkl\")\n",
    "\n",
    "# twentyone = model.encode_sentences(devign_func[20000:21000], combine_strategy = \"mean\", batch_size = len(devign_func[20000:21000]))\n",
    "# pickle_data(twentyone, \"twentyone.pkl\")\n",
    "\n",
    "# twentytwo = model.encode_sentences(devign_func[21000:22000], combine_strategy = \"mean\", batch_size = len(devign_func[21000:22000]))\n",
    "# pickle_data(twentytwo, \"twentytwo.pkl\")\n",
    "\n",
    "# twentythree = model.encode_sentences(devign_func[22000:23000], combine_strategy = \"mean\", batch_size = len(devign_func[22000:23000]))\n",
    "# pickle_data(twentythree, \"twentythree.pkl\")\n",
    "\n",
    "# twentyfour = model.encode_sentences(devign_func[23000:24000], combine_strategy = \"mean\", batch_size = len(devign_func[23000:24000]))\n",
    "# pickle_data(twentyfour, \"twentyfour.pkl\")\n",
    "\n",
    "# twentyfive = model.encode_sentences(devign_func[24000:25000], combine_strategy = \"mean\", batch_size = len(devign_func[24000:25000]))\n",
    "# pickle_data(twentyfive, \"twentyfive.pkl\")\n",
    "\n",
    "# twentysix = model.encode_sentences(devign_func[25000:26000], combine_strategy = \"mean\", batch_size = len(devign_func[25000:26000]))\n",
    "# pickle_data(twentysix, \"twentysix.pkl\")\n",
    "\n",
    "# twentyseven = model.encode_sentences(devign_func[26000:27000], combine_strategy = \"mean\", batch_size = len(devign_func[26000:27000]))\n",
    "# pickle_data(twentyseven, \"twentyseven.pkl\")\n",
    "\n",
    "# twentyeight = model.encode_sentences(devign_func[27000:], combine_strategy = \"mean\", batch_size = len(devign_func[27000:]))\n",
    "# pickle_data(twentyeight, \"twentyeight.pkl\")\n",
    "\n",
    "\n",
    "# next 7++, slice by num starts small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that D2A uses a global split, and that is what we used\n",
    "\n",
    "# Always use We follow the original split for each dataset, but if the split information is \n",
    "# unavailable we split the dataset 80/10/10 (training/validation/testing). (from VulBERT)\n",
    "\n",
    "one = unpickle_data(\"/Users/Gabriel/Projects/OuR/our/features/one.pkl\")\n",
    "two = unpickle_data(\"/Users/Gabriel/Projects/OuR/our/features/two.pkl\")\n",
    "three = unpickle_data(\"/Users/Gabriel/Projects/OuR/our/features/three.pkl\")\n",
    "four = unpickle_data(\"/Users/Gabriel/Projects/OuR/our/features/four.pkl\")\n",
    "five = unpickle_data(\"/Users/Gabriel/Projects/OuR/our/features/five.pkl\")\n",
    "d2a_train_features = np.concatenate((one, two, three, four, five), axis = 0)\n",
    "\n",
    "d2a_dev_features = unpickle_data(\"/Users/Gabriel/Projects/OuR/our/features/dev.pkl\")\n",
    "len(d2a_dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = svm.SVC() # 0.6191275167785235\n",
    "# svc.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = svc.predict(X_test)\n",
    "# print(accuracy_score(y_test, y_pred)) # 50 something\n",
    "\n",
    "# mlp = MLPClassifier() # 53.0000\n",
    "# mlp = MLPClassifier(max_iter=5000)\n",
    "# mlp.fit(d2a_train_features, d2a_train_labels)\n",
    "\n",
    "# rf = RandomForestClassifier() # lower than 53\n",
    "# rf.fit(d2a_train_features, d2a_train_labels)\n",
    "\n",
    "svc = svm.SVC() # 0.6191275167785235\n",
    "svc.fit(d2a_train_features, d2a_train_labels)\n",
    "\n",
    "# lr = LogisticRegression(solver='lbfgs', max_iter=4000) # 0.565\n",
    "# lr.fit(d2a_train_features, d2a_train_labels)\n",
    "\n",
    "# nb = GaussianNB() # 0.5620805369127517\n",
    "# nb.fit(d2a_train_features, d2a_train_labels)\n",
    "\n",
    "y_pred = svc.predict(d2a_dev_features)\n",
    "print(accuracy_score(d2a_dev_labels, y_pred)*100) #  accuracy_score(labels, preds)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc # = 0.5285234899328859 (Not Good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reaval_train = unpickle_data(\"/Users/Gabriel/Projects/OUR/data/reveal/reveal_train.pkl\")\n",
    "reaval_val = unpickle_data(\"/Users/Gabriel/Projects/OUR/data/reveal/reveal_val.pkl\")\n",
    "reaval_test = unpickle_data(\"/Users/Gabriel/Projects/OUR/data/reveal/reveal_test.pkl\")\n",
    "\n",
    "vuldeepecker_train = unpickle_data(\"/Users/Gabriel/Projects/OUR/data/vuldeepecker/vuldeepecker_train.pkl\")\n",
    "vuldeepecker_train.shape\n",
    "# vuldeepecker_train[[\"functionSource\"]]\n",
    "\n",
    "# devign\n",
    "devign_func, devign_targets  =  ([] for i in range(2))\n",
    "devign = json_r(\"/Users/Gabriel/Projects/OUR/data/devign/devign.json\")\n",
    "for data in devign:\n",
    "    devign_func.append(data[\"func\"]), devign_targets.append(data[\"target\"])\n",
    "len(devign_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27318"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(devign_gpt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_txtfile(file):\n",
    "#     with open(file) as f:\n",
    "#         return f.readlines()\n",
    "\n",
    "# devign_train_ids = [int(idx.replace(\"\\n\", \"\")) for idx in read_txtfile(\"/Users/Gabriel/Projects/OuR/data/devign/ids/train.txt\")]\n",
    "# devign_val_ids = [int(idx.replace(\"\\n\", \"\")) for idx in read_txtfile(\"/Users/Gabriel/Projects/OuR/data/devign/ids/valid.txt\")]\n",
    "# devign_test_ids = [int(idx.replace(\"\\n\", \"\")) for idx in read_txtfile(\"/Users/Gabriel/Projects/OuR/data/devign/ids/test.txt\")]\n",
    "# temp_ids = devign_train_ids + devign_val_ids + devign_test_ids\n",
    "\n",
    "\n",
    "# dsd_prep = sorted([file for file in dsd_dir.iterdir() if file.suffix == \".pkl\"], reverse = True)\n",
    "\n",
    "devign_dir = \"/Users/Gabriel/Projects/OuR/our/features/devign\"\n",
    "full_paths, file_names = [], []\n",
    "for root, _, files in os.walk(devign_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pkl\"):\n",
    "            temp_path = os.path.join(root, file)\n",
    "            full_paths.append(temp_path)\n",
    "            file_names.append(int(pathlib.Path(temp_path).stem.split(\"-\")[0])) # next load files\n",
    "out = {}\n",
    "for f_path, f_name in zip(full_paths, file_names):\n",
    "    out[f_name] = unpickle_data(f_path)\n",
    "temp_features = list(OrderedDict(sorted(out.items())).values())\n",
    "devign_gpt_features = []\n",
    "for data in temp_features:\n",
    "    for i in data:\n",
    "        devign_gpt_features.append(i)\n",
    "        \n",
    "# devign_gpt_features[0][0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(devign_gpt_features, devign_targets, test_size = 0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5583821376281113\n"
     ]
    }
   ],
   "source": [
    "# Devign Experiments\n",
    "\n",
    "# mlp = MLPClassifier() # 53.0000\n",
    "# mlp = MLPClassifier(max_iter=20000) # 0.5547218155197657\n",
    "# mlp.fit(X_train, y_train)\n",
    "\n",
    "# rf = RandomForestClassifier() # lower than 53\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# svc = svm.SVC()\n",
    "# svc.fit(X_train, y_train)\n",
    "\n",
    "# lr = LogisticRegression(solver='lbfgs', max_iter=10000) \n",
    "# lr.fit(X_train, y_train)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = svc.predict(X_val) # 0.5722781335773102 (X_val-SVC)\n",
    "# print(accuracy_score(y_val, y_pred))\n",
    "\n",
    "y_pred = nb.predict(X_test) # 0.5620424597364568 (X_test-SVC)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvn_train_ids = reader(\"/Users/Gabriel/Projects/OUR/data/devign\", \"train.txt\")\n",
    "dvn_validation_ids = reader(\"/Users/Gabriel/Projects/OUR/data/devign\", \"valid.txt\")\n",
    "dvn_test_ids = reader(\"/Users/Gabriel/Projects/OUR/data/devign\", \"test.txt\")\n",
    "devign = json_r(\"/Users/Gabriel/Projects/OUR/data/devign/devign.json\")\n",
    "\n",
    "functions, targets  =  ([] for i in range(2))\n",
    "ffmeg, targets  =  ([] for i in range(4))\n",
    "\n",
    "for data in devign:\n",
    "    functions.append(data[\"func\"]), targets.append(data[\"target\"])\n",
    "    ffmeg.append(data[\"FFmpeg\"])\n",
    "\n",
    "targets = np.array(targets)\n",
    "\n",
    "#  Write data to ..\n",
    "def write_data(location, idx = 1, data):\n",
    "    with open(os.path.join(location, f\"{idx}.c\"), \"w\") as cfile:\n",
    "        cfile.write(data)\n",
    "\n",
    "def c_gen(location, data):\n",
    "    for idx, file in enumerate(data):\n",
    "        write_data(location, idx + 1, file)\n",
    "\n",
    "location = \"/Users/Gabriel/Projects/OUR/data/devign/functions/2\"\n",
    "\n",
    "\n",
    "# len(devign) = 27318\n",
    "# len(devign) = 27318 // 2 = 13659\n",
    "# devign[0].keys() # type -> dict\n",
    "# functions[:len(functions)//2] # split range\n",
    "# functions[len(functions)/ 2:]\n",
    "\n",
    "# C paths\n",
    "# train_pth=`ls $PWD/d2a_raw/train/*.c` \n",
    "# dev_pth=`ls $PWD/d2a_raw/dev/*.c`\n",
    "# test_pth=`ls $PWD/d2a_raw/test/*.c`\n",
    "\n",
    "# devign=`ls /Users/Gabriel/Projects/OUR/data/devign/functions/1/*.c`\n",
    "\n",
    "functions[:len(functions)//2]\n",
    "functions[len(functions)//2:]\n",
    "c_gen(location, functions[len(functions)//2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fature extraction from generated ASTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split = 80% train, 10% dev, 10% test\n",
    "\n",
    "# \n",
    "d2a_train = pd.read_csv(\"/Users/Gabriel/Projects/OUR/data/d2a/function/train.csv\")\n",
    "d2a_dev = pd.read_csv(\"/Users/Gabriel/Projects/OUR/data/d2a/function/dev.csv\")\n",
    "d2a_test = pd.read_csv(\"/Users/Gabriel/Projects/OUR/data/d2a/function/test.csv\")\n",
    "\n",
    "d2a_train_sourcefiles = list(d2a_train[\"code\"])\n",
    "d2a_dev_sourcefiles = list(d2a_dev[\"code\"])\n",
    "d2a_test_sourcefiles = list(d2a_test[\"code\"])\n",
    "\n",
    "d2a_train_labels = np.array(list(d2a_train[\"label\"]))\n",
    "d2a_dev_labels = np.array(list(d2a_dev[\"label\"]))\n",
    "\n",
    "def get_asts(project):\n",
    "    out = {}\n",
    "    for root, _, files in os.walk(project):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                source_file_pth = os.path.join(root, file) \n",
    "                source_file_name = int(pathlib.Path(source_file_pth).stem.split(\".\")[0])\n",
    "                try:\n",
    "                    with open(source_file_pth, \"r\") as source_file:\n",
    "                        out[source_file_name] = source_file.read()\n",
    "                except OSError as e:\n",
    "                    raise e\n",
    "    return list(OrderedDict(sorted(out.items())).values())\n",
    "\n",
    "# ===== extract asts ========\n",
    "# D2A \n",
    "d2a_train_asts = get_asts(\"/Users/Gabriel/Projects/OUR/data/d2a/raw/train\")\n",
    "d2a_dev_asts = get_asts(\"/Users/Gabriel/Projects/OUR/data/d2a/raw/dev\")\n",
    "d2a_test_asts = get_asts(\"/Users/Gabriel/Projects/OUR/data/d2a/raw/test\")\n",
    "\n",
    "# Devign\n",
    "devign_asts1 = get_asts(\"/Users/Gabriel/Projects/OUR/data/devign/functions/1\")\n",
    "devign_asts2 = get_asts(\"/Users/Gabriel/Projects/OUR/data/devign/functions/2\")\n",
    "\n",
    "devign_asts = devign_asts1 + devign_asts2\n",
    "\n",
    "# extract features\n",
    "# D2A First\n",
    "def extract_features(observations, features): # main\n",
    "    nodes = []\n",
    "    for observation in observations:\n",
    "        temp = []\n",
    "#         temp = data.split()\n",
    "        for data in observation.split():\n",
    "            if data.endswith(features):\n",
    "                temp.append(data)\n",
    "        nodes.append(temp)\n",
    "    out = []\n",
    "    for node in nodes:\n",
    "        temp = []\n",
    "        for feature in node:\n",
    "            temp.append(feature.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "        out.append(temp)\n",
    "    return out\n",
    "\n",
    "d2a_stmts = extract_features(d2a_train_asts, \"Stmt\")\n",
    "d2a_exprs = extract_features(d2a_train_asts, \"Expr\")\n",
    "d2a_decls = extract_features(d2a_train_asts, \"Decl\")\n",
    "\n",
    "# dev\n",
    "dev_d2a_stmts = extract_features(d2a_dev_asts, \"Stmt\")\n",
    "dev_d2a_exprs = extract_features(d2a_dev_asts, \"Expr\")\n",
    "dev_d2a_decls = extract_features(d2a_dev_asts, \"Decl\")\n",
    "\n",
    "# stmt\n",
    "d2a_stmts_sequence = []\n",
    "for feature in d2a_stmts:\n",
    "    d2a_stmts_sequence.append(\" \".join([str(data) for data in feature]))\n",
    "\n",
    "# expr\n",
    "d2a_exprs_sequence = []\n",
    "for feature in d2a_exprs:\n",
    "    d2a_exprs_sequence.append(\" \".join([str(data) for data in feature]))\n",
    "    \n",
    "# decl\n",
    "d2a_decls_sequence = []\n",
    "for feature in d2a_decls:\n",
    "    d2a_decls_sequence.append(\" \".join([str(data) for data in feature]))\n",
    "    \n",
    "# devign_stmts = extract_features(devign_asts, \"Stmt\")\n",
    "# devign_exprs = extract_features(devign_asts, \"Expr\")\n",
    "# devign_decls = extract_features(devign_asts, \"Decl\")\n",
    "\n",
    "# devign_types = extract_features(devign, \"Type\")\n",
    "# devign_opts = extract_features(devign, \"Operator\")\n",
    "# devign_ltrls = extract_features(devign, \"Literal\")\n",
    "\n",
    "\n",
    "############### DEV SET #################\n",
    "dev_d2a_exprs_sequence = []\n",
    "for feature in dev_d2a_exprs:\n",
    "    dev_d2a_exprs_sequence.append(\" \".join([str(data) for data in feature]))\n",
    "\n",
    "# dev_d2a_stmts, dev_d2a_exprs, dev_d2a_decls\n",
    "# okay so I joined it and not treated as individual list, GOOD\n",
    "dev_d2a_decls_sequence = []\n",
    "for feature in dev_d2a_decls:\n",
    "    dev_d2a_decls_sequence.append(\" \".join([str(data) for data in feature]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with no node selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RepresentationModel(\n",
    "        model_type = \"gpt2\",\n",
    "        model_name = \"gpt2\",\n",
    "        use_cuda = False)\n",
    "\n",
    "# =========== stmts starts\n",
    "oneK_d2a_stmts = model.encode_sentences(d2a_stmts_sequence[:1000], combine_strategy = \"mean\", batch_size = len(d2a_stmts_sequence[:1000]))\n",
    "pickle_data(oneK_d2a_stmts, \"oneK_d2a_stmts.pkl\")\n",
    "\n",
    "twoK_d2a_stmts = model.encode_sentences(d2a_stmts_sequence[1000:2000], combine_strategy = \"mean\", batch_size = len(d2a_stmts_sequence[1000:2000]))\n",
    "pickle_data(twoK_d2a_stmts, \"twoK_d2a_stmts.pkl\")\n",
    "\n",
    "threeK_d2a_stmts = model.encode_sentences(d2a_stmts_sequence[2000:3000], combine_strategy = \"mean\", batch_size = len(d2a_stmts_sequence[2000:3000]))\n",
    "pickle_data(threeK_d2a_stmts, \"threeK_d2a_stmts.pkl\")\n",
    "\n",
    "fourK_d2a_stmts = model.encode_sentences(d2a_stmts_sequence[3000:4000], combine_strategy = \"mean\", batch_size = len(d2a_stmts_sequence[3000:4000]))\n",
    "pickle_data(fourK_d2a_stmts, \"fourK_d2a_stmts.pkl\")\n",
    "\n",
    "fiveK_d2a_stmts = model.encode_sentences(d2a_stmts_sequence[4000:], combine_strategy = \"mean\", batch_size = len(d2a_stmts_sequence[3000:4000]))\n",
    "pickle_data(fiveK_d2a_stmts, \"fiveK_d2a_stmts.pkl\")\n",
    "\n",
    "# =========== exprs starts\n",
    "oneK_d2a_exprs = model.encode_sentences(d2a_stmts_sequence[:1000], combine_strategy = \"mean\", batch_size = len(d2a_stmts_sequence[:1000]))\n",
    "pickle_data(oneK_d2a_stmts, \"oneK_d2a_stmts.pkl\")\n",
    "\n",
    "twoK_d2a_exprs = model.encode_sentences(d2a_exprs_sequence[1000:2000], combine_strategy = \"mean\", batch_size = len(d2a_exprs_sequence[1000:2000]))\n",
    "pickle_data(twoK_d2a_exprs, \"twoK_d2a_exprs.pkl\")\n",
    "\n",
    "threeK_d2a_exprs = model.encode_sentences(d2a_exprs_sequence[2000:3000], combine_strategy = \"mean\", batch_size = len(d2a_exprs_sequence[2000:3000]))\n",
    "pickle_data(threeK_d2a_exprs, \"threeK_d2a_exprs.pkl\")\n",
    "\n",
    "fourK_d2a_exprs = model.encode_sentences(d2a_exprs_sequence[3000:4000], combine_strategy = \"mean\", batch_size = len(d2a_exprs_sequence[3000:4000]))\n",
    "pickle_data(fourK_d2a_exprs, \"fourK_d2a_exprs.pkl\")\n",
    "\n",
    "fiveK_d2a_exprs = model.encode_sentences(d2a_exprs_sequence[4000:], combine_strategy = \"mean\", batch_size = len(d2a_exprs_sequence[4000:]))\n",
    "pickle_data(fiveK_d2a_exprs, \"fiveK_d2a_exprs.pkl\")\n",
    "\n",
    "# =========== decls starts\n",
    "oneK_d2a_stmts = model.encode_sentences(d2a_stmts_sequence[:1000], combine_strategy = \"mean\", batch_size = len(d2a_stmts_sequence[:1000]))\n",
    "pickle_data(oneK_d2a_stmts, \"oneK_d2a_stmts.pkl\")\n",
    "\n",
    "# more\n",
    "# # make vocab dynamic\n",
    "# def sr(features, source_files, vocab):\n",
    "#     frequencies = []\n",
    "#     for data in features:\n",
    "#         frequencies.append(len([True for d in data if d in vocab]))\n",
    "#     out = [] \n",
    "#     for source_file, frequency in zip(source_files, frequencies):\n",
    "#         with np.errstate(divide = \"ignore\"):\n",
    "#             data_point = -np.log10(frequency / len(source_file))\n",
    "#             out.append(0.0) if math.isinf(data_point) else out.append(data_point)\n",
    "#     return out\n",
    "\n",
    "frequencies = []\n",
    "for data in devign_stmts:\n",
    "    frequencies.append(len([True for d in data if d in stmts_vocab]))\n",
    "    \n",
    "# devign - functions\n",
    "stmts_vocab = set([feature for features in devign_stmts for feature in features])\n",
    "exprs_vocab = set([feature for features in devign_exprs for feature in features])\n",
    "decls_vocab = set([feature for features in devign_decls for feature in features])\n",
    "\n",
    "stmts_vectors = sr(devign_stmts, functions, stmts_vocab)\n",
    "exprs_vectors = sr(devign_exprs, functions, exprs_vocab)\n",
    "decls_vectors = sr(devign_decls, functions,decls_vocab)\n",
    "\n",
    "feature_vectors = np.array((list(zip(stmts_vectors, exprs_vectors, decls_vectors))))\n",
    "\n",
    "# def perf_measure(y_actual, y_hat):\n",
    "#     TP, FP, TN, FN = 0\n",
    "#     for i in range(len(y_hat)): \n",
    "#         if y_actual[i]==y_hat[i]==1:\n",
    "#            TP += 1\n",
    "#         if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "#            FP += 1\n",
    "#         if y_actual[i]==y_hat[i]==0:\n",
    "#            TN += 1\n",
    "#         if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "#            FN += 1\n",
    "\n",
    "#     return(TP, FP, TN, FN)\n",
    "\n",
    "# TP, FP, TN, FN = perf_measure(y_test, y_pred)\n",
    "# precision = TP / (TP + FP)\n",
    "# recall = TP / (TP + FN)\n",
    "# f1 = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "# print(f1)\n",
    "\n",
    "def extract_features(data_obs, features):\n",
    "    out = []\n",
    "    for data in data_obs:\n",
    "        t_out = []\n",
    "        temp = data.split()\n",
    "        for t in temp:\n",
    "            if t.endswith(features):\n",
    "                t_out.append(t)\n",
    "        out.append(t_out)\n",
    "    return out\n",
    "\n",
    "# Extract Features\n",
    "d2a_train_stmts, d2a_train_exprs = extract_features(d2a_train_asts, \"Stmt\"), extract_features(d2a_train_asts, \"Expr\")\n",
    "d2a_train_decls, d2a_train_types  = extract_features(d2a_train_asts, \"Decl\"), extract_features(d2a_train_asts, \"Type\")\n",
    "\n",
    "d2a_dev_stmts = extract_features(d2a_dev_asts, \"Stmt\")\n",
    "d2a_dev_exprs = extract_features(d2a_dev_asts, \"Expr\")\n",
    "d2a_dev_decls = extract_features(d2a_dev_asts, \"Decl\")\n",
    "d2a_dev_types = extract_features(d2a_dev_asts, \"Type\")\n",
    "\n",
    "d2a_test_stmts = extract_features(d2a_test_asts, \"Stmt\")\n",
    "d2a_test_exprs = extract_features(d2a_test_asts, \"Expr\")\n",
    "d2a_test_decls = extract_features(d2a_test_asts, \"Decl\")\n",
    "d2a_test_types = extract_features(d2a_test_asts, \"Type\")\n",
    "\n",
    "# Remove Punctuation\n",
    "def rmv_punc(feature_nodes):\n",
    "    out = []\n",
    "    for feature_node in feature_nodes:\n",
    "        temp = []\n",
    "        for feature in feature_node:\n",
    "            temp.append(feature.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "        out.append(temp)\n",
    "    return out\n",
    "\n",
    "# Train\n",
    "d2a_train_stmts, d2a_train_exprs = rmv_punc(d2a_train_stmts), rmv_punc(d2a_train_exprs) \n",
    "d2a_train_decls, d2a_train_types  = rmv_punc(d2a_train_decls), rmv_punc(d2a_train_types)\n",
    "\n",
    "# Dev\n",
    "d2a_dev_stmts, d2a_dev_exprs = rmv_punc(d2a_dev_stmts), rmv_punc(d2a_dev_exprs)\n",
    "d2a_dev_decls, d2a_dev_types = rmv_punc(d2a_dev_decls), rmv_punc(d2a_dev_types)\n",
    "\n",
    "# Test\n",
    "d2a_test_stmts, d2a_test_exprs = rmv_punc(d2a_test_stmts), rmv_punc(d2a_test_exprs)\n",
    "d2a_test_decls = rmv_punc(d2a_test_decls)\n",
    "d2a_test_types = rmv_punc(d2a_test_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features(observations, features): # main\n",
    "#     nodes = []\n",
    "#     for observation in observations:\n",
    "#         temp = []\n",
    "# #         temp = data.split()\n",
    "#         for data in observation.split():\n",
    "#             if data.endswith(features):\n",
    "#                 temp.append(data)\n",
    "#         nodes.append(temp)\n",
    "#     out = []\n",
    "#     for node in nodes:\n",
    "#         temp = []\n",
    "#         for feature in node:\n",
    "#             temp.append(feature.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "#         out.append(temp)\n",
    "#     return out\n",
    "\n",
    "# devign_stmts, devign_exprs = extract_features(d2a_train_asts, \"Stmt\"), extract_features(d2a_train_asts, \"Expr\")\n",
    "# d2a_train_decls, d2a_train_types  = extract_features(d2a_train_asts, \"Decl\"), extract_features(d2a_train_asts, \"Type\")\n",
    "\n",
    "\n",
    "# def vocabulary(feature_nodes):\n",
    "#     return set([i for feature_node in feature_nodes for i in feature_node])\n",
    "\n",
    "# stmts_train_vocab, stmts_dev_vocab, stmts_test_vocab = vocabulary(d2a_train_stmts), vocabulary(d2a_dev_stmts), vocabulary(d2a_test_stmts) \n",
    "# exprs_train_vocab, exprs_dev_vocab, exprs_test_vocab = vocabulary(d2a_train_exprs), vocabulary(d2a_dev_exprs), vocabulary(d2a_test_exprs) \n",
    "# decls_train_vocab, decls_dev_vocab, decls_test_vocab = vocabulary(d2a_train_decls), vocabulary(d2a_dev_decls), vocabulary(d2a_test_decls)\n",
    "# types_train_vocab, types_dev_vocab, types_test_vocab = vocabulary(d2a_train_types), vocabulary(d2a_dev_types), vocabulary(d2a_test_types)\n",
    "\n",
    "# stmts_vocab = stmts_train_vocab.union(stmts_dev_vocab, stmts_test_vocab) # vocab size = 15\n",
    "# exprs_vocab = exprs_train_vocab.union(exprs_dev_vocab, exprs_test_vocab) # vocab size = 15\n",
    "# decls_vocab = decls_train_vocab.union(decls_dev_vocab, decls_test_vocab) # vocab size = 9 \n",
    "# types_vocab = types_train_vocab.union(types_dev_vocab, types_test_vocab) # vocab size = 12\n",
    "\n",
    "# vocabulary_size = list(stmts_vocab) + list(exprs_vocab) + list(decls_vocab) + list(types_vocab)\n",
    "# # len(vocabulary_size)  = 51\n",
    "\n",
    "def sr(features, source_files, vocab):\n",
    "    frequencies = []\n",
    "    for data in features:\n",
    "        frequencies.append(len([True for d in data.split() if d.translate(str.maketrans(\"\", \"\", string.punctuation)) in vocab]))\n",
    "    out = [] \n",
    "    for source_file, frequency in zip(source_files, frequencies):\n",
    "        with np.errstate(divide = \"ignore\"):\n",
    "            data_point = -np.log10(frequency / len(source_file))\n",
    "            out.append(0.0) if math.isinf(data_point) else out.append(data_point)\n",
    "    return out\n",
    "\n",
    "# # d2a_train_sourcefiles, d2a_dev_sourcefiles, d2a_test_sourcefiles\n",
    "# train_stmts_fetures = sr(d2a_train_asts, d2a_train_sourcefiles, stmts_vocab)\n",
    "# train_exprs_fetures = sr(d2a_train_asts, d2a_train_sourcefiles, exprs_vocab)\n",
    "# train_decls_fetures = sr(d2a_train_asts, d2a_train_sourcefiles, decls_vocab)\n",
    "# # train_types_fetures = sr(d2a_train_asts, d2a_train_sourcefiles, types_vocab)\n",
    "\n",
    "# dev_stmts_fetures = sr(d2a_dev_asts, d2a_dev_sourcefiles, stmts_vocab)\n",
    "# dev_exprs_fetures = sr(d2a_dev_asts, d2a_dev_sourcefiles, exprs_vocab)\n",
    "# dev_decls_fetures = sr(d2a_dev_asts, d2a_dev_sourcefiles, decls_vocab)\n",
    "# # dev_types_fetures = sr(d2a_dev_asts, d2a_dev_sourcefiles, types_vocab)\n",
    "\n",
    "# sr_train_X = np.array(list(zip(train_stmts_fetures, train_exprs_fetures, train_decls_fetures)))\n",
    "# sr_dev_X = np.array(list(zip(dev_stmts_fetures, dev_exprs_fetures, dev_decls_fetures)))\n",
    "\n",
    "# # sr_train_X = np.concatenate((train_stmts_fetures, train_exprs_fetures, train_decls_fetures), axis = 1)\n",
    "# # sr_dev_X = np.concatenate((dev_stmts_fetures, dev_exprs_fetures, dev_decls_fetures), axis = 1)\n",
    "\n",
    "# # _train_X = np.concatenate((_train_stmts_embeds, _train_exprs_embeds, _train_decls_embeds), axis = 1)\n",
    "# # _dev_X = np.concatenate((_dev_stmts_embeds, _dev_exprs_embeds, _dev_decls_embeds), axis = 1)\n",
    "\n",
    "# Model Size\n",
    "# FOR DL (CNN) Model Size, see pytorch link & See specdet cnn model, check size after training and validation\n",
    "\n",
    "mlp = MLPClassifier() \n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 20,034 is model size\n",
    "\n",
    "# Once you have a model, you can print the number of parameters in the millions in \n",
    "# pytorch using print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "# return the size in bytes. NOT PARAMETERS - SO BAD X\n",
    "\n",
    "# mlp = pickle.dumps(mlp)\n",
    "# print(sys.getsizeof(mlp)) \n",
    "\n",
    "mlp.get_params(deep=True)\n",
    "\n",
    "# Explicitly specifiy mlp arch, with layers and all\n",
    "\n",
    "# np.random.seed(10)\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# knn = KNeighborsClassifier()\n",
    "mlp = MLPClassifier()\n",
    "# svc = svm.SVC()\n",
    "\n",
    "start_time = time.time()\n",
    "# rf.fit(sr_train_X, train_y)\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"MLP Fit: {time.time() - start_time} sec\")\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = svc.predict(sr_dev_X)\n",
    "# print(f\"PRT: {time.time() - start_time} sec\")\n",
    "\n",
    "# # F1 = f1_score(dev_y, y_pred)\n",
    "# # Precision = precision_score(dev_y, y_pred)\n",
    "# # Recall = recall_score(dev_y, y_pred)\n",
    "# acc = accuracy_score(dev_y, y_pred)\n",
    "# print(f\"Accuracy: {acc}\")\n",
    "\n",
    "# print(f\"F1: {F1}\")\n",
    "# print(f\"Pre: {Precision}\")\n",
    "# print(f\"Rec: {Recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_model(data, name):\n",
    "    model = Word2Vec(sentences = data, vector_size = 32).wv\n",
    "    model.save(name)\n",
    "\n",
    "def embed_lookup(vec, data):\n",
    "    vec_dict = {}\n",
    "    model = KeyedVectors.load(vec, mmap = \"r\")\n",
    "    for key in model.key_to_index.keys():\n",
    "        vec_dict[key] = model[key]\n",
    "    out = []\n",
    "    for embeds in data:\n",
    "        temp = []\n",
    "        for embed in embeds:\n",
    "            if embed in vec_dict:\n",
    "                temp.append(vec_dict[embed])\n",
    "        out.append(temp)\n",
    "    return out\n",
    "\n",
    "def _model(data):\n",
    "    vec_dict = {}\n",
    "    model = Word2Vec(sentences = data, vector_size = 32).wv\n",
    "    for key in model.key_to_index.keys():\n",
    "        vec_dict[key] = model[key]\n",
    "    out = []\n",
    "    for embeds in data:\n",
    "        temp = []\n",
    "        for embed in embeds:\n",
    "            if embed in vec_dict:\n",
    "                temp.append(vec_dict[embed])\n",
    "        out.append(temp)\n",
    "    return out\n",
    "\n",
    "def truncate(vectors):\n",
    "    return [vector[:64] if len(vector) > 64 else vector for vector in vectors]\n",
    "\n",
    "def pad(embeddings):\n",
    "    out  = []\n",
    "    for embedding in embeddings:\n",
    "        if len(embedding) < 64:\n",
    "            embedding.extend([0.0] * (64 - len(embedding)))\n",
    "    for embedding in embeddings:\n",
    "        temp = []\n",
    "        for vector in embedding:\n",
    "            if not type(vector).__module__ == np.__name__:\n",
    "                temp.append(np.zeros((32,), dtype = np.float64))\n",
    "            else: temp.append(vector)\n",
    "        out.append(temp)\n",
    "    return out\n",
    "\n",
    "def flatten(data):\n",
    "    out = []\n",
    "    for vector_list in data:\n",
    "        flatten_list = np.concatenate(vector_list).ravel().tolist()\n",
    "        out.append(flatten_list) \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Stylometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Types\n",
    "_train_stmts_embeds, _train_exprs_embeds = flatten(pad(truncate(_model(d2a_train_stmts)))), flatten(pad(truncate(_model(d2a_train_exprs))))\n",
    "_train_decls_embeds, _train_types_embeds = flatten(pad(truncate(_model(d2a_train_decls)))), flatten(pad(truncate(_model(d2a_train_types))))\n",
    "\n",
    "_dev_stmts_embeds, _dev_exprs_embeds = flatten(pad(truncate(_model(d2a_dev_stmts)))), flatten(pad(truncate(_model(d2a_dev_exprs))))\n",
    "_dev_decls_embeds, _dev_types_embeds = flatten(pad(truncate(_model(d2a_dev_decls)))), flatten(pad(truncate(_model(d2a_dev_types))))\n",
    "\n",
    "train_X = np.concatenate((_train_stmts_embeds, _train_exprs_embeds, _train_decls_embeds, _train_types_embeds), axis = 1)\n",
    "dev_X = np.concatenate((_dev_stmts_embeds, _dev_exprs_embeds, _dev_decls_embeds, _dev_types_embeds), axis = 1)\n",
    "\n",
    "\n",
    "# With No Types -- MAIN\n",
    "_train_stmts_embeds = flatten(pad(truncate(_model(d2a_train_stmts))))\n",
    "_train_exprs_embeds = flatten(pad(truncate(_model(d2a_train_exprs))))\n",
    "_train_decls_embeds  = flatten(pad(truncate(_model(d2a_train_decls))))\n",
    "\n",
    "_dev_stmts_embeds = flatten(pad(truncate(_model(d2a_dev_stmts))))\n",
    "_dev_exprs_embeds = flatten(pad(truncate(_model(d2a_dev_exprs))))\n",
    "_dev_decls_embeds = flatten(pad(truncate(_model(d2a_dev_decls))))\n",
    "\n",
    "_train_X = np.concatenate((_train_stmts_embeds, _train_exprs_embeds, _train_decls_embeds), axis = 1)\n",
    "_dev_X = np.concatenate((_dev_stmts_embeds, _dev_exprs_embeds, _dev_decls_embeds), axis = 1)\n",
    "\n",
    "# np.random.seed(10)\n",
    "mlp = MLPClassifier()\n",
    "start_time = time.time()\n",
    "mlp.fit(_train_X, train_y)\n",
    "print(f\"MLP Fit: {time.time() - start_time} sec\")\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred = mlp.predict(_dev_X)\n",
    "print(f\"PRT: {time.time() - start_time} sec\")\n",
    "\n",
    "# F1 = f1_score(dev_y, y_pred)\n",
    "# Precision = precision_score(dev_y, y_pred)\n",
    "# Recall = recall_score(dev_y, y_pred)\n",
    "acc = accuracy_score(dev_y, y_pred)\n",
    "\n",
    "# print(f\"F1: {F1}\")\n",
    "# print(f\"Pre: {Precision}\")\n",
    "# print(f\"Rec: {Recall}\")\n",
    "print(f\"Acc: {acc}\")\n",
    "\n",
    "#### Word Piece Tokenization\n",
    "\n",
    "# def wpt(features):\n",
    "#     tokenizer, out = BertTokenizer.from_pretrained(\"bert-base-uncased\"), []\n",
    "#     for feature in features:\n",
    "#         out.append(\" \".join([str(data) for data in feature]))\n",
    "#     return [tokenizer.tokenize(data) for data in out]\n",
    "\n",
    "# train_stmts, train_exprs, train_decls, train_types = wpt(d2a_train_stmts), wpt(d2a_train_exprs), wpt(d2a_train_decls),  wpt(d2a_train_types)\n",
    "# train_features_wpt = train_stmts + train_exprs + train_decls + train_types\n",
    "\n",
    "# dev_stmts, dev_exprs, dev_decls, dev_types = wpt(d2a_dev_stmts), wpt(d2a_dev_exprs), wpt(d2a_dev_decls),  wpt(d2a_dev_types)\n",
    "# dev_features_wpt = dev_stmts + dev_exprs + dev_decls + dev_types\n",
    "\n",
    "# all_features_wpt = train_features_wpt + dev_features_wpt\n",
    "\n",
    "# # Train Lookup\n",
    "# train_stmts_embeds, train_exprs_embeds = embed_lookup(\"embeddings.kv\", train_stmts), embed_lookup(\"embeddings.kv\", train_exprs)\n",
    "# train_decls_embeds, train_types_embeds = embed_lookup(\"embeddings.kv\", train_decls), embed_lookup(\"embeddings.kv\", train_types)\n",
    "\n",
    "# # Train Truncate and Pad and Flatten\n",
    "# train_stmts_embeds, train_exprs_embeds = flatten(pad(truncate(train_stmts_embeds))), flatten(pad(truncate(train_exprs_embeds)))\n",
    "# train_decls_embeds, train_types_embeds = flatten(pad(truncate(train_decls_embeds))), flatten(pad(truncate(train_types_embeds)))\n",
    "\n",
    "# # =================================================================================================================================================\n",
    "# # Dev lookup\n",
    "# dev_stmts_embeds, dev_exprs_embeds = embed_lookup(\"embeddings.kv\", dev_stmts), embed_lookup(\"embeddings.kv\", dev_exprs)\n",
    "# dev_decls_embeds, dev_types_embeds = embed_lookup(\"embeddings.kv\", dev_decls), embed_lookup(\"embeddings.kv\", dev_types)\n",
    "\n",
    "# # Dev Truncate and Pad and Flatten\n",
    "# dev_stmts_embeds, dev_exprs_embeds = flatten(pad(truncate(dev_stmts_embeds))), flatten(pad(truncate(dev_exprs_embeds)))\n",
    "# dev_decls_embeds, dev_types_embeds = flatten(pad(truncate(dev_decls_embeds))), flatten(pad(truncate(dev_types_embeds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
